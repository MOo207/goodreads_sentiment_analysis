GOODREADS REVIEW ANALYSIS PRESENTATION
==================================================
Generated on: 2025-11-09 13:21:02

DATASET OVERVIEW
==================================================
Dataset Statistics:
  Total Reviews: 2438
  Unique Sentiments: 3
  Avg Review Length: 2575.08 characters
  Sentiment Distribution: positive: 1670, negative: 534, neutral: 234

Sample Data Before Cleaning:
------------------------------
                                              review  rating
0  It's written 1948? Clearly History has its twi...     5.0
1  حدثني عن القهر عن الاستعباد عن الذل ثم حدثني ب...     5.0
2  YOU. ARE. THE. DEAD. Oh my God. I got the chil...     5.0

Sample Data After Cleaning:
------------------------------
After cleaning, the dataset contains only English reviews with sentiment labels:
  - Positive sentiment (4-5 stars): 1552 reviews
  - Neutral sentiment (3 stars): 215 reviews
  - Negative sentiment (1-2 stars): 525 reviews

Sample Data After Text Preprocessing:
------------------------------
Original: 'This book was absolutely fantastic! I couldn't put it down.'
Processed: 'book absolut fantast could put'

Preprocessing steps applied:
  - Convert to lowercase
  - Remove punctuation and numbers
  - Tokenize text
  - Remove stopwords
  - Apply stemming

METHODOLOGY SUMMARY
==================================================
METHODOLOGY DESCRIPTION
=====================

1. DATA PREPROCESSING
   - Language filtering: Only English reviews are retained
   - Text cleaning: Removal of punctuation, numbers, and special characters
   - Tokenization: Breaking text into individual words
   - Stopword removal: Common words that don't contribute to sentiment
   - Stemming/Lemmatization: Reducing words to their root forms
   - Negation handling: Preserving sentiment context in negated phrases

2. SENTIMENT CLASSIFICATION
   - Converting 5-star ratings to 3 sentiment categories:
     * 1-2 stars: Negative sentiment
     * 3 stars: Neutral sentiment
     * 4-5 stars: Positive sentiment

3. FEATURE EXTRACTION
   - Bag of Words (BOW): Traditional word count approach
   - TF-IDF: Term frequency-inverse document frequency for word importance
   - Sequential features: For deep learning models

4. MODEL TRAINING
   - Traditional ML models: Naive Bayes, Logistic Regression, SVM
   - Deep Learning models: LSTM and SimpleRNN for sequence modeling
   - Cross-validation: Ensuring robust model evaluation

5. MODEL EVALUATION
   - Accuracy metrics for comparing different approaches
   - Performance analysis explaining why some models work better


FLOWCHART OVERVIEW
==================================================

    DATA PROCESSING PIPELINE FLOWCHART
    ================================
    
    1. Data Collection
       |
       ├── Load 88 CSV files from dataset
       └── Combine into unified dataframe
    
    2. Data Preprocessing
       |
       ├── Remove rows with missing reviews/ratings
       ├── Convert ratings to numeric values
       ├── Clean text data (remove punctuation, numbers)
       ├── Tokenize text
       ├── Remove stopwords
       └── Apply stemming
    
    3. Data Preparation
       |
       ├── Split data into train/test sets
       ├── Encode labels (ratings 1-5)
       ├── Prepare features using:
       |   ├── Bag of Words
       |   ├── TF-IDF
       |   └── Sequences for RNN
       └── Normalize data where needed
    
    4. Model Training & Evaluation
       |
       ├── Train multiple classifiers:
       |   ├── Naive Bayes
       |   ├── Logistic Regression
       |   ├── SVM
       |   └── RNN (LSTM/SimpleRNN)
       ├── Evaluate with accuracy, precision, recall
       └── Compare model performance
    
    5. Results Analysis
       |
       ├── Select best performing model
       ├── Generate classification reports
       └── Create visualizations
    

MODEL COMPARISON TABLES
==================================================

Model Performance Results:
--------------------------------------------------
Model                               Accuracy  
--------------------------------------------------
✗ Multinomial Naive Bayes (BOW)     0.6667    
✓ Logistic Regression (BOW)         0.7432    
✗ SVM (BOW)                         0.6776    
✗ Multinomial Naive Bayes (TF-IDF)  0.6858    
✓ Logistic Regression (TF-IDF)      0.7568    
✓ SVM (TF-IDF)                      0.765     
✓ LSTM RNN                          0.724     
✗ SimpleRNN                         0.6858    


Model Parameters Comparison:
--------------------------------------------------------------------------------
Model Type           Feature Extraction   Key Parameters                     
--------------------------------------------------------------------------------
Naive Bayes          Bag of Words         Default parameters                 
Logistic Regression  Bag of Words         max_iter=1000                      
SVM                  Bag of Words         kernel=linear                      
Naive Bayes          TF-IDF               Default parameters                 
Logistic Regression  TF-IDF               max_iter=1000                      
SVM                  TF-IDF               kernel=linear                      
LSTM RNN             Sequences            LSTM_UNITS=128, dropout=0.5        
SimpleRNN            Sequences            LSTM_UNITS=128, dropout=0.5        

DETAILED ANALYSIS
==================================================
Performance Statistics:
  Mean Accuracy: 0.7131
  Best Accuracy: 0.7650
  Worst Accuracy: 0.6667
  Models above 70%: 4/8

Best performing model: SVM (TF-IDF) with accuracy 0.7650

Performance by Model Category:
----------------------------------------
Bag of Words Models - Average: 0.70%
TF-IDF Models - Average: 0.74%
RNN Models - Average: 0.70%

BENCHMARKING SUMMARY
==================================================
Model Performance Rankings:
------------------------------
1. SVM (TF-IDF): 0.77%
2. Logistic Regression (TF-IDF): 0.76%
3. Logistic Regression (BOW): 0.74%
4. LSTM RNN: 0.72%
5. SimpleRNN: 0.69%
6. Multinomial Naive Bayes (TF-IDF): 0.69%
7. SVM (BOW): 0.68%
8. Multinomial Naive Bayes (BOW): 0.67%

Performance Categories:
-------------------------
Excellent (≥75%): 2 models
Good (70-75%): 2 models
Fair (<70%): 4 models

Best Performing Approach:
-------------------------
Feature Extraction: TF-IDF
Model Type: SVM
Accuracy: 77.03%

VISUALIZATION SUMMARY
==================================================
1. Data Pipeline Visualization (data_pipeline.png)
   - Shows review counts at each processing stage
   - Demonstrates impact of data cleaning steps

2. Presentation Visualizations (presentation_visualizations.png)
   - Sentiment distribution charts
   - Review length distribution histograms
   - Model comparison bar charts
   - Average review length by sentiment

3. Model Performance Comparison (model_performance_comparison.png)
   - Bar plot comparison of all models
   - Horizontal bar chart with performance metrics

4. Detailed Model Comparison (detailed_model_comparison.png)
   - Categorized model performance visualization
   - Color-coded by model type