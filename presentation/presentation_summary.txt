GOODREADS REVIEW ANALYSIS PRESENTATION
=====================================

1. DATASET OVERVIEW
===================

Dataset Statistics:
- Total Reviews: 2,292
- Unique Sentiments: 3 (Positive, Neutral, Negative)
- Average Review Length: 2,565.44 characters
- Sentiment Distribution:
  * Positive (4-5 stars): 1,552 reviews
  * Negative (1-2 stars): 525 reviews
  * Neutral (3 stars): 215 reviews

Sample Data Before Cleaning:
review                                                rating
"It's written 1948? Clearly History has its twi..."   5.0
"حدثني عن القهر عن الاستعباد عن الذل ثم حدثني ب..."   5.0
"YOU. ARE. THE. DEAD. Oh my God. I got the chil..."   5.0

Sample Data After Cleaning:
- Non-English reviews filtered out
- Only English reviews retained
- Sentiment labels applied based on star ratings

Sample Data After Text Preprocessing:
Original: "This book was absolutely fantastic! I couldn't put it down."
Processed: "book absolut fantast could put"

Preprocessing steps:
1. Convert to lowercase
2. Remove punctuation and numbers
3. Tokenize text
4. Remove stopwords
5. Apply stemming

2. METHODOLOGY
==============

1. DATA PREPROCESSING
   - Language filtering: Only English reviews are retained
   - Text cleaning: Removal of punctuation, numbers, and special characters
   - Tokenization: Breaking text into individual words
   - Stopword removal: Common words that don't contribute to sentiment
   - Stemming/Lemmatization: Reducing words to their root forms
   - Negation handling: Preserving sentiment context in negated phrases

2. SENTIMENT CLASSIFICATION
   - Converting 5-star ratings to 3 sentiment categories:
     * 1-2 stars: Negative sentiment
     * 3 stars: Neutral sentiment
     * 4-5 stars: Positive sentiment

3. FEATURE EXTRACTION
   - Bag of Words (BOW): Traditional word count approach
   - TF-IDF: Term frequency-inverse document frequency for word importance
   - Sequential features: For deep learning models

4. MODEL TRAINING
   - Traditional ML models: Naive Bayes, Logistic Regression, SVM
   - Deep Learning models: LSTM and SimpleRNN for sequence modeling
   - Cross-validation: Ensuring robust model evaluation

5. MODEL EVALUATION
   - Accuracy metrics for comparing different approaches
   - Performance analysis explaining why some models work better

3. FLOWCHART
============

1. Data Collection
   |
   ├── Load 88 CSV files from dataset
   └── Combine into unified dataframe

2. Data Preprocessing
   |
   ├── Remove rows with missing reviews/ratings
   ├── Convert ratings to numeric values
   ├── Clean text data (remove punctuation, numbers)
   ├── Tokenize text
   ├── Remove stopwords
   └── Apply stemming

3. Data Preparation
   |
   ├── Split data into train/test sets
   ├── Encode labels (ratings 1-5)
   ├── Prepare features using:
   |   ├── Bag of Words
   |   ├── TF-IDF
   |   └── Sequences for RNN
   └── Normalize data where needed

4. Model Training & Evaluation
   |
   ├── Train multiple classifiers:
   |   ├── Naive Bayes
   |   ├── Logistic Regression
   |   ├── SVM
   |   └── RNN (LSTM/SimpleRNN)
   ├── Evaluate with accuracy, precision, recall
   └── Compare model performance

5. Results Analysis
   |
   ├── Select best performing model
   ├── Generate classification reports
   └── Create visualizations

4. MODEL COMPARISON TABLES
==========================

Model Performance Results:
Model                                    Accuracy
-----------------------------------------------
✓ SVM (TF-IDF)                           0.7703
✓ Logistic Regression (BOW)              0.7674
✓ Multinomial Naive Bayes (BOW)          0.7645
✓ Logistic Regression (TF-IDF)           0.7442
✓ SVM (BOW)                              0.7326
✓ LSTM RNN                               0.7035
✗ Multinomial Naive Bayes (TF-IDF)       0.6773
✗ SimpleRNN                              0.6773

Model Parameters Comparison:
Model Type              Feature Extraction    Key Parameters
-----------------------------------------------------------
Naive Bayes             Bag of Words          Default parameters
Logistic Regression     Bag of Words          max_iter=1000
SVM                     Bag of Words          kernel=linear
Naive Bayes             TF-IDF                Default parameters
Logistic Regression     TF-IDF                max_iter=1000
SVM                     TF-IDF                kernel=linear
LSTM RNN                Sequences             LSTM_UNITS=128, dropout=0.5
SimpleRNN               Sequences             LSTM_UNITS=128, dropout=0.5

5. DETAILED ANALYSIS
====================

Performance Statistics:
- Mean Accuracy: 0.7296
- Best Accuracy: 0.7703 (SVM with TF-IDF)
- Worst Accuracy: 0.6773 (Multinomial Naive Bayes with TF-IDF and SimpleRNN)
- Models above 70%: 6 out of 8

Performance by Model Category:
- Bag of Words Models - Mean Accuracy: 0.7548
- TF-IDF Models - Mean Accuracy: 0.7306
- RNN Models - Mean Accuracy: 0.6904

6. DATASET LABELING AND AUGMENTATION
====================================

Dataset Labeling:
The dataset was labeled using the star ratings from Goodreads reviews:
- 1-2 stars: Negative sentiment
- 3 stars: Neutral sentiment
- 4-5 stars: Positive sentiment

Dataset Augmentation Techniques:
1. Language Filtering:
   - Non-English reviews were filtered out using langdetect library
   - This improved data quality by ensuring linguistic consistency

2. Text Preprocessing:
   - Removal of punctuation and numbers
   - Tokenization
   - Stopword removal
   - Stemming

3. Feature Engineering:
   - Bag of Words representation
   - TF-IDF representation
   - Sequential representation for RNN models

4. Data Balancing:
   - SMOTE + Tomek Links applied to handle class imbalance
   - This improved model performance on minority classes

7. VISUALIZATIONS
=================

Created Visualizations:
1. model_performance_comparison.png
   - Bar chart comparing all model performances
   - Horizontal bar chart with color coding

2. detailed_model_comparison.png
   - Detailed horizontal bar chart with model categorization
   - Color-coded by model type

3. data_pipeline.png
   - Visualization of the data processing pipeline
   - Shows the number of reviews at each stage

4. presentation_visualizations.png
   - Original visualizations from app.py
   - Sentiment distribution
   - Review length distribution
   - Model comparison
   - Average review length by sentiment

8. ENHANCEMENT RECOMMENDATIONS
==============================

To further improve model performance, consider the following enhancements:

1. Advanced Text Preprocessing:
   - Implement spell correction using pyspellchecker
   - Convert emojis to text representations
   - Handle negations more effectively

2. Improved Feature Extraction:
   - Use Complement Naive Bayes for imbalanced datasets
   - Expand TF-IDF features to 15K with n-grams up to 3
   - Implement Word2Vec embeddings to capture semantic relationships

3. Ensemble Methods:
   - Create soft voting ensembles combining NB, LR, and SVM models
   - Implement stacked ensembles with meta-learners

4. Hyperparameter Optimization:
   - Perform systematic hyperparameter tuning using GridSearchCV
   - Use cross-validation for robust evaluation

5. Advanced Models:
   - Implement Bidirectional LSTM for better sequence modeling
   - Try CNN models for text classification
   - Experiment with hybrid CNN-LSTM architectures